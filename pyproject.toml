[build-system]
requires = ["hatchling>=1.18"]
build-backend = "hatchling.build"

[project]
name = "llm-eval-guardrails"
version = "0.1.0"
description = "LLM evaluation harness with guardrails, adversarial testing, and reporting."
readme = "README.md"
requires-python = ">=3.11"
license = { file = "LICENSE" }
authors = [
  { name = "LLM Evaluation Team", email = "engineering@example.com" }
]
dependencies = [
  "typer[all]>=0.9.0",
  "pydantic>=2.6.0",
  "rich>=13.7.0",
  "jinja2>=3.1.3",
  "plotly>=5.18.0",
  "pandas>=2.2.0",
  "numpy>=1.26.0",
  "python-dotenv>=1.0.0",
  "chromadb>=0.4.24",
  "sentence-transformers>=2.2.2",
  "httpx>=0.25.0",
  "pyyaml>=6.0.0",
  "tqdm>=4.66.0"
]

[project.optional-dependencies]
guardrails = ["guardrails-ai>=0.5.6"]
eval = ["trulens-eval>=0.9.4", "ragas>=0.1.5"]
toxicity = ["transformers>=4.36.0", "torch>=2.1.0", "scikit-learn>=1.3.0"]

dev = [
  "black>=24.4.0",
  "ruff>=0.4.0",
  "mypy>=1.9.0",
  "pytest>=8.1.0",
  "pytest-cov>=5.0.0",
  "types-PyYAML>=6.0.12.12",
  "pre-commit>=3.5.0"
]

[project.scripts]
evalguard = "evalguard.cli:app"

[tool.black]
line-length = 100
target-version = ["py311"]

[tool.ruff]
line-length = 100
select = ["E", "F", "W", "B", "I", "N"]
ignore = ["E203", "E501"]
target-version = "py311"

[tool.mypy]
python_version = "3.11"
warn_unused_configs = true
warn_unused_ignores = true
warn_redundant_casts = true
warn_return_any = true
strict = true
pretty = true
ignore_missing_imports = true

[tool.pytest.ini_options]
minversion = "8.0"
addopts = "-q --disable-warnings"
testpaths = ["tests"]

[tool.hatch.build.targets.wheel]
packages = ["src/evalguard"]

[tool.hatch.build.targets.sdist]
include = ["src/evalguard", "README.md", "LICENSE", "requirements.txt", "pyproject.toml", "data", "tests", ".github", "Makefile", "pre-commit-config.yaml"]
